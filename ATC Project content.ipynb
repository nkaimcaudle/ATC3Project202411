{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c09c26-4826-46e4-8bc4-497b75a0b82d",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5edab6d-915c-4156-a82d-98f5d0135e8d",
   "metadata": {},
   "source": [
    "My trading strategy consists of option trading on the S&P 500 via the ES listed options and related futures contracts. The basic premise is to reconstruct the volatility surface using hidden latent factors and then trade arbitrage opportunities where the observed market volatility is far away from the reconstructed one. Rather than using PCA for the decomposition I am using a deep neural network architecture called a Variational AutoEncoder, more specifically a Conditional Variational AutoEncoder, CVAE. Using a CVAE allows for a non-linear decomposition of the equity vol surface. \n",
    "\n",
    "A trading signal is generated if the reconstructed volatility surface is materially different from the observed one. \n",
    "\n",
    "The factors are modelled on the log-strike space of the volatility surface so once a trading signal is sent it will be necessary to choose an optimal actual listed option with known strike and expiry (or multiple options). As this strategy is a volatility based strategy it will be delta hedged to zero using futures at the end of each trading day. The signal is tested once per day near the closing time of the regular trading session. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081b1963-3525-47a9-824b-7432d0c30b55",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6a6718-4f19-485c-b195-f454c095c6eb",
   "metadata": {},
   "source": [
    "## Data\n",
    "The source of my data for ES futures and options is [DataBento](https://databento.com). For the futures I obtain the OHLCV bars by minute over the trading day, for the options I obtain the best bid and ask price as of 12 minutes before the close of the S&P, at 15:48 US/Eastern time. DataBento don't expilicity provide a snapshot functionality so it is necessary to obtain all the top of book updates, I do this for each listed option for the 10 minutes prior to 15:48 in each trading session. \n",
    "\n",
    "The CVAE model expects data in implied volatility format so it is first necessary to work out the implied fair futures level and discount factor on each trading day for each listed expiration. Once obtained each top of book quote can then be converted into an implied vol using the Black 1976 formula and a root finding algorithm. Using raw quote information leads to some difficulties which need to be handled carefully. For instance\n",
    "\n",
    "For the fair discount rate box prices were used (a synthetic on a lower strike minus a synthetic on a higher strike, so 4 options in total) as they have no sensitivity to implied vol nor future level. For each expiration all the possible box bids and asks were collected and average was taken where the mid implied rate was above the highest bid and below the lowest ask rate. For this a Bayesian approach with markov chain monte-carlo was used. \n",
    "\n",
    "Similarly for the fair futures level, once the discount rate was found, a Bayesian approach was used on all the synthetic quotes mids, bids and asks. Once the discount rate and fair future level was worked out the implied vol for bid, ask and mid could easily be backed out. \n",
    "\n",
    "The time period is from 02-June-2017 until 25-Oct-2024, however there are gaps where there is not enough quotes to build a representative volatility surface. For each trading day we only count it as valid if there are at least 3 option expirations where:\n",
    "1. Minimum is log_strike is atleast less than -0.5\n",
    "2. Maximum of log_strike is atleast greater than 0.15\n",
    "3. There are atleast 6 seperate strikes\n",
    "\n",
    "For a strike to be valid it must have a bid and and ask. The ES option market changed over the time period studied, near the start there are only ever 4 expirations with generally less than 1 year to expiration. Near the end of the study some of the expirations are 3 years from pricing date. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6f4bbb-b5e2-4fc0-a763-d4458d173539",
   "metadata": {},
   "source": [
    "## Volatility Latent Model\n",
    "Here we use a Conditional Variational Autoencoder (CVAE) architecture designed for time series analysis. The model consists of an encoder-decoder framework that incorporates conditional\n",
    "information to guide both the encoding and decoding processes.\n",
    "\n",
    "The encoder network transforms input data X and conditional variables y through a series of fully connected layers with SiLU (Sigmoid Linear Unit) activation functions. The architecture employs a dimensional\n",
    "reduction strategy, mapping the input through hidden layers to produce two outputs: a mean vector (μ) and log-variance vector (logvar) that parameterize the latent space distribution.\n",
    "\n",
    "The latent space representation is obtained through the reparameterization trick, z = μ + σ ∗ ε, where σ = exp(0.5 ∗ logvar) and ε is sampled from a standard normal distribution. This ensures differentiability\n",
    "during training while maintaining the stochastic nature of the encoding process.\n",
    "\n",
    "The decoder network takes the latent vector z concatenated with the conditional variables y and reconstructs the input through a mirror architecture of fully connected layers. The final output layer uses a\n",
    "softplus activation function to ensure positive outputs, this ensures that the volatility surface is strictly positive and hence valid.\n",
    "\n",
    "The conditional information passed along to the Encoder and Decoder in the variable y is the number of years from the pricing date to the expiration. EQ options and futures have fixed expirations, e.g. 20-Dec-2024. So at each pricing date the number of days left until expiry slightly reduces and this will impact the volatility surface. By supplying this y vector into the CVAE it allows the network to learn the relationship between time and volatility rather than having it externally modelled. \n",
    "\n",
    "The training objective combines three components:\n",
    "1. Reconstruction loss: Mean squared error between input and reconstructed output\n",
    "2. KL divergence loss: Ensures the latent space distribution approximates a standard normal distribution\n",
    "3. Correlation loss: Minimizes the correlation between latent dimensions, promoting independence in the latent representation\n",
    "\n",
    "The model implements time series cross-validation using weighted averaging as described by Donate et al., with weights following a geometric progression that favours more recent performance. The implementation\n",
    "leverages JAX for automatic differentiation and hardware acceleration, with Equinox providing the neural network modules.\n",
    "\n",
    "Notable features include input normalization, configurable network dimensions, and comprehensive metric tracking including latent space statistics. The architecture is particularly suited for conditional\n",
    "generation tasks where external variables influence the underlying data distribution.\n",
    "\n",
    "Other items to note:\n",
    "1. The SiLU activation function is used so that we get non-zero gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee62e25-0da9-4622-8b65-8067701d330b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5518d08c-abad-4fef-82ef-7de316dbac24",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddc0571-c90b-4429-a1d4-8119e0607809",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e243f-725e-47d5-847b-f005e1182dd2",
   "metadata": {},
   "source": [
    "# Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c9687-a6aa-44bf-bda1-77eaf4e3e1e1",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c6627-8517-4acc-9ab7-443e73c0ce95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c748f68-59a8-4032-9492-5a8d638b2b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb486c55-eb3e-43ac-a19a-61b38f25e3df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164562ed-1f95-45a4-8158-3cea6886c262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127463f3-e0d1-4dfc-88d3-35f67c6611b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e7b2d5-898e-4300-bee2-96aca07763a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8aba6f-f160-4794-9446-50a31bdabf09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac688942-9be4-4188-bc5c-54a92fe643a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93c753a9-b0c8-45a1-be1a-261e49566270",
   "metadata": {},
   "source": [
    "**OLD**\n",
    "\n",
    "This implementation presents a Conditional Variational Autoencoder (CVAE) architecture designed to work with two-dimensional input data conditioned on auxiliary information. The model employs a hybrid\n",
    "convolutional-dense architecture with the following key components:\n",
    "\n",
    "Encoder Architecture: The encoder network consists of a sequential structure that processes both the input data and conditional information:\n",
    "\n",
    " 1. A 2D convolutional layer (Conv2d) with kernel size (2,3) that processes single-channel input data, mapping it to out_channels feature maps\n",
    " 2. A flattening operation followed by concatenation with the conditional vector y\n",
    " 3. A fully connected layer with SiLU (Sigmoid Linear Unit) activation that maps to a hidden dimension\n",
    " 4. Two parallel linear layers that output the mean (μ) and log-variance (log σ²) of the latent space distribution\n",
    "\n",
    "Decoder Architecture: The decoder network mirrors the encoder's structure in reverse:\n",
    "\n",
    " 1. A fully connected layer that processes the concatenated latent vector and conditional information\n",
    " 2. An intermediate dense layer with SiLU activation\n",
    " 3. A reshape operation to prepare for transposed convolution\n",
    " 4. A transposed convolution layer (ConvTranspose2d) with kernel size (2,3) that reconstructs the original input dimensions\n",
    " 5. A final Softplus activation ensuring non-negative output values\n",
    "\n",
    "Latent Space: The model implements the standard VAE reparameterization trick for sampling from the latent space: z = μ + ε * σ, where ε ~ N(0,1)\n",
    "\n",
    "Loss Function: The training objective appears to be a composite loss function combining:\n",
    "\n",
    " 1. A reconstruction term (likely mean squared error, based on the continuous nature of the data)\n",
    " 2. A KL divergence term weighted by kl_loss_alpha\n",
    " 3. A correlation loss term weighted by correl_loss_alpha\n",
    "\n",
    "Implementation Details:\n",
    "\n",
    " • The model is implemented using the Equinox framework, leveraging JAX for automatic differentiation and acceleration\n",
    " • The architecture supports variable input dimensions through parameterized height and width\n",
    " • The model maintains dimensional consistency through careful sizing of the convolutional and dense layers\n",
    " • Training employs a batched approach with configurable batch sizes and learning rates\n",
    "\n",
    "Notable Features:\n",
    "\n",
    " 1. The use of SiLU activation functions in the intermediate layers, which have been shown to perform well in deep learning applications\n",
    " 2. The implementation of a hybrid architecture combining both convolutional and dense layers\n",
    " 3. The flexibility to adjust the capacity of the model through configurable parameters:\n",
    "    - Latent dimension size\n",
    "    - Hidden dimension size\n",
    "    - Number of convolutional channels\n",
    "    - Input dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb0375b-bdea-4cdd-a386-f463d61fdb08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08818162-6154-4ee1-818a-d6359a6067cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e258b1-5a5b-40f7-83c1-ebc4fe850721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f11082-3aef-4241-96d8-f0d05c7b9ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bfb5eb9-e3db-4310-b5e0-1b193dc529c1",
   "metadata": {},
   "source": [
    "This implementation presents a Conditional Variational Autoencoder (CVAE) designed for time series data analysis. The architecture combines convolutional and dense layers, processing both input data and\n",
    "conditional information through an encoder-decoder structure.\n",
    "\n",
    "The encoder processes input through a 2D convolutional layer followed by dense layers, outputting parameters (mean and log-variance) of the latent space distribution. The decoder reconstructs the input from\n",
    "the latent representation using transposed convolutions, with both encoder and decoder incorporating the conditional information.\n",
    "\n",
    "The model employs a composite loss function with two components:\n",
    "\n",
    " 1. Reconstruction loss using mean squared error\n",
    " 2. KL divergence term to regularize the latent space\n",
    "\n",
    "Training utilizes time series cross-validation with 5 folds, implementing a weighted averaging scheme following Donate et al.'s formula. The implementation uses the JAX/Equinox framework for efficient\n",
    "computation and includes configurable hyperparameters for model capacity (latent dimension, hidden dimension, number of channels) and training dynamics (learning rate, loss weights).\n",
    "\n",
    "The architecture is particularly suited for temporal data where maintaining sequential relationships is crucial while ensuring decorrelated latent representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a63bd00-c6d9-438d-b2c5-0a9aa3ed6978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7855b2-3224-4b9f-92c7-ec0453df159d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d93120d-c556-4d42-9ad7-98e3dcc4d1f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7637cf69-bee1-44c4-81d4-66d525cad742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "decd5d59-1351-4725-962f-4d552281e2ba",
   "metadata": {},
   "source": [
    "### Further Research\n",
    "#### Backtesting Implementation\n",
    "- Use of margin for listed futures and options, end of day and intraday margin calls\n",
    "\n",
    "#### CVAE Model Architecture\n",
    "- Without Convolutional layers\n",
    "- More fully connected layers, wide and deep\n",
    "- Normalisation techniques such as BatchNormalisation\n",
    "- Dropout and random GaussianNoise layers\n",
    "\n",
    "#### Volatility Surface\n",
    "- Use normalised log-strikes, take sqrt(t) into account\n",
    "- Account for known large move event days from the economic calender, e.g. CPI, GDP, NFP, Fed, elections\n",
    "- Use business time rather than calendar time\n",
    "\n",
    "#### Data\n",
    "- Expand to cover multiple equity indices\n",
    "- Expand to cover stock universe\n",
    "- Use bid and ask vols rather than mid, perhaps could be another data item to pass into the network as a condition. -1 for bid, +1 for ask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a453d-c88a-4808-849f-b081a6fffc73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
